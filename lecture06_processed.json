[
  {
    "doc": "lecture06_processed.json",
    "id": "slide1",
    "text": "Naming in Distributed Systems\n• Concepts\n• Flat Naming\n• Structured Naming\n• Attribute-based Naming\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide2",
    "text": "NAMING\nNAMES, IDENTIFIERS, AND ADDRESSES \nFLAT NAMING \n\nSimple Solutions \n\nHome-Based Approaches \n\nDistributed Hash Tables (More in P2P)\n\nHierarchical Approaches \nSTRUCTURED NAMING \n\nName Spaces \n\nName Resolution \n\nThe Implementation of a Name Space \n\nExample: The Domain Name System \nATTRIBUTE-BASED NAMING \n\nDirectory Services \n\nHierarchical Implementations: LDAP \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide3",
    "text": "Objectives\nTo understand naming and related issues in DS\nTo learn naming space and implementation\nTo learn flat and structured names and how \nthey are resolved\nTo learn Attributed-based naming\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide4",
    "text": "What a Name is in DS?\nA name is a string of bits or characters that is used to refer \nto an entity (an entity could be anything  such as host, printer, file, process, \nmailbox, user etc.) \nTo operate on an entity, we need to access it, for which we \nneed an access point.\nAccess point is a special kind of entity and its name is called \nan address (address of the entity, e.g., IP, port #, phone #)\n\nAn entity may have more than one access point/address\n\nAn entity may change its access points/addresses \n\nSo using an address as a reference is inflexible and human \nunfriendly \n\nA better approach is to use a name that is location independent, \nmuch easier, and flexible to use\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide5",
    "text": "Identifier\nA special name to uniquely identify an entity (SSN, MAC)\nA true identifier has the following three properties:\n\nP1: Each identifier refers to at most one entity\n\nP2: Each entity is referred to by at most one identifier\n\nP3: An identifier always refers to same entity (no reuse)\n \n \nAddresses \n \nEntities \n \n \nIdentifiers\nI1\nI2\nI3\nE1\nE2\nE3\nA1\nA2\nA3\nA4\nAddresses and identifiers are important and used for different purposes, but \nthey are often represented in machine readable format (MAC, memory address)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide6",
    "text": "Human-friendly names\nFile names, variable names etc. are human-friendly names \ngiven to each entity\nQuestion: how to map/resolve these names to addresses \nso that we can access the entities on which we want to \noperate?\nSolution: have a naming system that maintains name-to-\naddress binding!\nThe simplest form is to have a centralized table!\n\nWhy or why not this will work?\nWe will study three different naming systems and how they \nmaintain such a table in a distributed manner!\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide7",
    "text": "Naming Systems and Their Goals\nNaming Systems \nFlat names\nStructured names\nAttributed-based names\nGoals\nScalable to arbitrary size\nHave a long lifetime\nBe highly available\nHave fault isolation\nTolerate mistrust\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide8",
    "text": "Flat Names\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide9",
    "text": "Flat Naming\nFlat name: random bits of string, no structure\n\nE.g., SSN, MAC address\nResolution problem: \n \nGiven a flat (unstructured) name, how can we find/locate \nits associated access point and its address?\nSolutions:\n\nSimple solutions (broadcasting)\n\nHome-based approaches\n\nDistributed Hash Tables (structured P2P)\n\nHierarchical location service\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide10",
    "text": "Simple Solution: Broadcasting \nSimply broadcast the target ID to every entity\nEach entity compares the requested ID with its own ID\nThe target entity returns its current address\nExample: \n\nRecall ARP in LAN\nAdv/Disadvantages\n\n+ simple\n\n- not scale beyond LANs \n\n- it requires all entities to listen to all incoming requests\nWho has the address \n192.168.0.1?\nI am 192.168.0.1. My identifier \nis 02:AB:4A:3C:59:85\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide11",
    "text": "Forwarding Pointers\nStub-Scion Pair (SSP) chains implement remote invocations for mobile \nentities using forwarding pointers\n\nServer stub is referred to as Scion in the original paper\nEach forwarding pointer is implemented as a pair: \n \n \n(client stub, server stub)\n\nThe server stub contains a local reference to the actual object or a local \nreference to another client stub\nWhen object moves from A (e.g., P2) to B (e.g., P3), \n\nIt leaves a client stub at A (i.e., P2)\n\nIt installs a server stub at B (i.e., P3)\nProcess P1\nProcess P2\nProcess P3\nProcess P4\n= Client stub\n= Server stub;\nn = Process n;\n= Remote Object;\n= Caller Object;\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide12",
    "text": "Forwarding Pointers\nHow to locate mobile entities?\nWhen an entity moves from A to B, leaves a pointer to A \nthat it is at B now… \nDereferencing: simply follow the chain of pointers and \nmake this entirely transparent to clients\nAdv/Disadvantages\n\n+ support for mobile nodes\n\n- geographical scalability problems\n\n- long chains are not fault tolerant\n\n- increased network latency\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide13",
    "text": "Home-Based Approaches\nHow to deal with scalability problem when locating mobile entities?\n\nLet a home keep track of where the entity is!\n\nHow will the clients continue to communicate? \n\nHome agent gives the new location to the client so it can directly \ncommunicate\n\nefficient but not transparent\n\nHome agent forwards the messages to new location\n\nTransparent but may not be efficient\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide14",
    "text": "Home-Based Approaches: An Example\nHome node\nMobile entity\n1. Update home node about the \nforeign address\n2. Client sends the packet to the \nmobile entity at its home node\n3a. Home node forwards the \nmessage to the foreign address \nof the mobile entity\n3b. Home node replies the client \nwith the  current IP address of \nthe mobile entity\n4. Client directly sends all \nsubsequent packets directly to the \nforeign address of the mobile entity\nFrom CS15-440 CMU Qatar, Hammoud\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide15",
    "text": "15\nProblems with home-based approaches\nThe home address has to be supported as long as the \nentity lives.\nThe home address is fixed, which means an \nunnecessary burden when the entity permanently \nmoves to another location\n\nHow can we solve the “permanent move” problem?\nPoor geographical scalability (the entity may be next to \nthe client)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide16",
    "text": "Next: Distributed Hash Table\nDistributed Hash Table – In a nutshell\nhttps://www.youtube.com/watch?v=tz-Q-eW8FbQ\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide17",
    "text": "Distributed Hash Tables\nHow to use DHT to resolve flat ID\nMany nodes into a logical ring\n\nEach node is assigned a random m-bit identifier.\n\nEvery entity is assigned a unique m-bit key.\n\nEntity with key k falls under jurisdiction of node with smallest id >= k \n(called its successor)\nLinearly resolve a key k to the address of succ(k)\n\nEach node p keeps two neighbors: \n \n succ(p+1) and pred(p)\n\nIf k > p  then \n \n  \nforward to succ(p+1)\n\nif k <= pred(p) then \n \n  \nforward k to pred(p)\n\nIf pred(p) < k <= p then\n \n return p’s address (p holds the entity)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide18",
    "text": "Chord\n\nChord assigns an m-bit identifier (randomly \nchosen) to each node\n\nA node can be contacted through its network \naddress\n\nAlongside, it maps each entity to a node\n\nEntities can be processes, files, etc.,\n\nMapping of entities to nodes\n\nEach node is responsible for a set of entities\n\nAn entity with key k falls under the jurisdiction of the \nnode with the smallest identifier id >= k. This \nnode is known as the successor of k, and is \ndenoted by succ(k)\nNode 000\nNode 005\nNode 010\nNode  301\n000\n003\n004\n008\n040\n079\nEntity \nwith k\nNode n (node with \nid=n)\nMap each entity with key k \nto node succ(k)\nFrom CS15-440 CMU Qatar, Hammoud\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide19",
    "text": "A Naïve Key Resolution Algorithm - Linearly\n\nThe main issue in DHT is to efficiently resolve a key k to the network location of succ(k)\n\nGiven an entity with key k, how to find the node succ(k)?\n1. All nodes are arranged in a logical \nring according to their IDs\n2. Each node ‘p’ keeps track of its \nimmediate neighbors: succ(p) and \npred(p)\n3. If ‘p’ receives a request to resolve \nkey ‘k’:\n• If pred(p) < k <=p, node p \nwill handle it\n• Else it will forward it to succ(n) \nor pred(n)\nn\n= Active node with id=n\np\n= No node assigned to key p\n19\nSolution is not scalable:\n•As the network grows, forwarding delays \nincrease\n•Key resolution has a time complexity of O(n)\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\nFrom CS15-440 CMU Qatar, Hammoud\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide20",
    "text": "\nChord improves key resolution by reducing the time complexity to O(log n)\n\nAll nodes are arranged in a logical ring according to their IDs\n\nEach node ‘p’ keeps a table FTp of at-most m entries. This table is Finger Table\n      FTp[i] = succ(p + 2(i-1))\nNOTE: FTp[i] increases exponentially\n\nIf node ‘p’ receives a request to resolve key ‘k’:\n• Node p will forward it to node q with index j in Fp where\n  q = FTp[j] <= k < FTp[j+1]\n• If k > FTp[m], then node p will forward it to FTp[m]\n• If k < FTp[1], then node p will forward it to FTp[1]\nDHT: Finger Table (Chord)\nHow to improve efficiency?\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide21",
    "text": "Chord DHT Example\n1\n04\n2\n04\n3\n09\n4\n09\n5\n18\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n1\n09\n2\n09\n3\n09\n4\n14\n5\n20\n1\n11\n2\n11\n3\n14\n4\n18\n5\n28\n1\n14\n2\n14\n3\n18\n4\n20\n5\n28\n1\n18\n2\n18\n3\n18\n4\n28\n5\n01\n1\n21\n2\n28\n3\n28\n4\n28\n5\n04\n1\n28\n2\n28\n3\n28\n4\n01\n5\n09\n1\n01\n2\n01\n3\n01\n4\n04\n5\n14\ni\nsucc(p + 2(i-1))\n26\n1.\nAll nodes are arranged in a logical \nring according to their IDs\n2.\nEach node ‘p’ keeps a table FTp of \nat-most m entries. This table is called \nFinger Table\n      FTp[i] = succ(p + 2(i-1))\nNOTE: FTp[i] increases \nexponentially\n \n3.\nIf node ‘p’ receives a request to \nresolve key ‘k’:\n• Node p will forward it to node q \nwith index j in Fp where\n  q = FTp[j] <= k < FTp[j+1]\n• If k > FTp[m], then node p \nwill forward it to FTp[m]\n• If k < FTp[1], then node p \nwill forward it to FTp[1]\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide22",
    "text": "DHT: Finger Table (cont’d)\n\nHow to handle\n\nJoin\n\nIt contacts arbitrary node, looks up for succ(p+1), and inserts itself into the ring\n\nLeave \n\nIt contacts pred(p) and succ(p+1) and updates them\n\nFail \n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n02\nWho is \nsucc(2+1) ?\nNode 4 is \nsucc(2+1)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide23",
    "text": "DHT: Finger Table (cont’d)\n\nThe complexity comes from keeping the finger tables up to date\n\nBy-and-large Chord tries to keep them consistent\n\nBut a simple mechanism may lead to performance problems\n\nTo fix this we need to exploit network proximity when assigning node ID\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide24",
    "text": "Exploiting network proximity\nProblem: The logical organization of nodes in the overlay may lead to \nerratic message transfers in the underlying Internet: node k and node \nsucc(k +1) may be very far apart.\n\nTopology-aware node assignment: \n \nWhen assigning an ID to a node, make sure that nodes close in the ID space are also \nclose in the network. Can be very difficult.\n\nProximity routing: \n \nMaintain more than one possible successor, and forward to the closest.\n \nExample: in Chord FTp[i] points to first node in INT = [p+2i−1,p+2i −1]. \n\nProximity neighbor selection: \n \nWhen there is a choice of selecting who your neighbor will be (not in Chord), pick the \nclosest one.\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide25",
    "text": "Hierarchical Location Services (HLS)\nA leaf domain, contained in S\nDirectory node  dir(S) of \ndomain S\nA subdomain S\nof top-level domain T  (S is \ncontained in T)\nTop-level  \ndomain T\nBasic idea\nBuild a large-scale search tree for which the underlying network is  \ndivided into hierarchical domains. Each domain is represented by a  \nseparate directory node.\nPrinciple\nnode dir(T)\nThe root directory\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide26",
    "text": "HLS: Tree organization\nDomain D2\nDomain D1\nLocation record  with \nonly one field,\ncontaining an address\nInvariants\n►Address of entity E is stored in a leaf or intermediate node\n►Intermediate nodes contain a pointer to a child if and only if the subtree \nrooted at the child stores an address of the entity\n►The root knows about all entities\nStoring information of an entity having two addresses in  different leaf \ndomains\ndom(N) with  \npointer to N\nLocation record  for E at \nnode M\nN\nField with no data\nM\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide27",
    "text": "HLS: Lookup operation\nBasic principles\n►Start lookup at local leaf node\n►Node knows about E ⇒ follow downward pointer, else go up\n►Upward lookup always stops at root\nLooking up a location\nDomain D\nM\nNode has no  \nrecord for E, so  \nthat request is  \nforwarded to  \nparent\nLook-up  \nrequest\nNode knows\nabout E, so request  \nis forwarded to child\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide28",
    "text": "Structured Naming\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide29",
    "text": "Name Space \nCollection of valid names\nA directed graph with two types of nodes\n\nLeaf node represents a (named) entity, has no outgoing link, and stores \ninformation about the entity (e.g., address)\n\nA directory node is an entity that refers to other nodes: contains a \n(directory) table of (edge label, node identifier) pairs\nroot\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide30",
    "text": "Name Space  (cont.)\nEach node in the graph is actually considered to be \nanother entity and we can easily store all kinds of \nattributes in a node, describing aspects of the entity the \nnode represents:\n\nType of the entity\n\nAn identifier for that entity \n\nAddress of the entity’s location \n\nNicknames\n\n… … \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide31",
    "text": "Name Resolution\nlooking up a name\nN: <label-1, label-2, …, label-n>\n \nStart at directory node N\n \n \nfind label-1 in directory table of N\n \n \nget the identifier\n \n \ncontinue resolving at that node until reaching label-n\n\nProblem: where to start? How do we actually find that (initial) node?\n\nClosure mechanism:  knowing how and where to start name \nresolution. It is always implicit. Why? \n\nInode in unix is the first block in logical disk\n\nwww.cs.vu.nl: start at a DNS name server\n\n/home/steen/mbox: start at the local NFS file server (possible recursive \nsearch)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide32",
    "text": "Name Resolution: Aliases and linking\nAlias is another name for the same entity.\nThere are 2 ways of aliasing in naming graphs\n\nHard Links: What we have described so far as a path name: a name that is \nresolved by following a specific path in a naming graph from one node to \nanother (i.e., there are more than one absolute paths to a certain node) \n\nSoft Links: We can represent an entity by a leaf node that stores an \nabsolute path name of another node. (like symbolic links in UNIX file \nsystem)\n\nNode O contains a name of another node:\n\nFirst resolve O’s name (leading to O)\n\nRead the content of O, yielding name\n\nName resolution continues with name \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide33",
    "text": "Name Linking\nThe name space can be effectively used to link two \ndifferent entities\nTwo types of links can exist between the nodes:\n1. Hard Links\n2. Symbolic Links\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide34",
    "text": "1. Hard Links\nThere is a directed link from \nthe hard link to the actual \nnode\nName resolution:\nSimilar to the general \nname resolution\nConstraint:\nThere should be no cycles \nin the graph\n“/home/steen/keys” is a hard link \nto “/keys”\nn0\nn1\nn4\nn5\nn2\nn3\nhome\nkeys\nsteen\nmax\nelke\n“/keys”\ntwmrc\nmbox\nkeys\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide35",
    "text": "2. Symbolic Links\nSymbolic link stores the name of \nthe original node as data\nName resolution for a symbolic link \nSL\nFirst resolve SL’s name\nRead the content of SL \nName resolution continues with \ncontent of SL\nConstraint:\nNo cyclic references should be \npresent\n“/home/steen/keys” is a \nsymbolic link to “/keys”\nn0\nn1\nn4\nn5\nn2\nn3\nhome\nkeys\nsteen\nmax\nelke\n“/keys”\ntwmrc mbox\nkeys\nn6\n“/keys”\nData stored in n6\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide36",
    "text": "Mounting of Name Spaces\n\nTwo or more name spaces can be merged transparently by a \ntechnique known as mounting\n\nWith mounting, a directory node in one name space will store the \nidentifier of the directory node of another name space\n\nNetwork File System (NFS) is an example where different name \nspaces are mounted\n\nNFS enables transparent access to remote files\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide37",
    "text": "Example of Mounting Name Spaces in NFS\nMachine B\nName Space 2\nOS\nMachine A\nName Space 1\nOS\nhome\nsteen\nmbox\nName Server \nfor \nforeign name \nspace\nremot\ne\nvu\n“nfs://flits.cs.vu.\nnl/home/steen”\nName resolution for “/remote/vu/home/steen/mbox” in a \ndistributed file system\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide38",
    "text": "Name Space \nImplementation\nDistributed vs. centralized \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide39",
    "text": "Basic issue: Distribute the name resolution process as \nwell as name space management across multiple \nmachines, by distributing nodes of the naming graph\nLarge name spaces are organized in a hierarchical way. \nThere are three logical layers\n\nGlobal level: Consists of the high-level directory nodes \nrepresenting different organizations or groups\n\nStable (directory tables don’t change often)\n\nHave to be jointly managed by different administrations\n\nAdministrational level: Contains mid-level directory nodes \nmanaged within a single organization \n\nRelatively stable\n\nManagerial level: Consists of low-level directory nodes within a \nsingle administration. \n\nNodes may change often, requiring effective mapping of names\n\nManaged by admins or users \nName Space Distribution\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide40",
    "text": "Name Space Distribution (cont.)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide41",
    "text": "Name Space Distribution (cont.)\nServers in each layer have different \nrequirements regarding availability and \nperformance\nAvailability\nPerformance\nGlobal\nMust be very high\nReplication may \nhelp\nCan be cached (stability)\nReplication may help\nAdministrat\nive\nMust be very high \nparticularly for the \nclients in the same \norganization\nLooks up should be fast\nUse high-end machines\nManagerial\nLess demanding\nOne dedicated \nserver might be \nenough\nPerformance is crucial\nOperations should take \nplace immediately\nCaching would not be eff. \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide42",
    "text": "Implementation of Name Resolution\nIterative \n \n \n   vs. \n \nRecursive\n-Caching is restricted to client\n-Communication cost, Delay \n+ less overhead on root\n+Caching can be more effective\n+Communication cost might be reduced\n- Too much overhead on root \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide43",
    "text": "Cache in Recursive Naming Resolution\nRecursive name resolution of <nl, vu, cs, ftp>. \nName servers cache intermediate results for subsequent \nlookups\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide44",
    "text": "Scalability Issues\n\nSize scalability: We need to ensure that servers can handle a large \nnumber of requests per time unit à high-level servers are in big \ntrouble.\n\nSolution: Assume (at least at global and administrational level) that \ncontent of nodes hardly ever changes. In that case, we can apply \nextensive replication by mapping nodes to multiple servers, and start \nname resolution at the nearest server.\n\nGeographical scalability: We need to ensure that the name \nresolution process scales across large geographical distances.\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide45",
    "text": "Case Study: Domain \nName System (DNS)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide46",
    "text": "Case Study: Domain Name System (DNS)\nOne of the largest distributed naming database/service\nThe DNS name space is hierarchically organized as a \nrooted tree. Name structure reflects administrative \nstructure of the Internet\nRapidly resolves domain names \nto IP addresses\n\nexploits caching heavily\n\ntypical query time ~100 milliseconds\nScales to millions of computers\n\npartitioned database\n\ncaching\nResilient to failure of a server\n\nreplication\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide47",
    "text": "Domain names (last element of name)\n\ncom - commercial organizations\n\nedu - universities and educational institutions\n\ngov - US government agencies\n\nmil - US military organizations\n\nnet - major network support centers\n\norg - organizations not included in first five\n\nint - international organization\n\ncountry codes - (e.g., cn, us, uk, fr, etc.)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide48",
    "text": "Name spaces in DNS\nHierarchical structure - one or more components \nor labels separated by periods (.)\nOnly absolute names - referred relative to global \nroot\nClients usually have a list of default domains that \nare appended to single-component domain \nnames before trying global root\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide49",
    "text": "Zone partitioning of DNS name space\nZone - contains attribute data for names in domain minus \nthe sub-domains administrated by lower-level authorities:\nNames of the servers for the sub-domains\nAt least two name servers that provide authoritative \ndata for the zone\nZone management parameters: cache, replication\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide50",
    "text": "Authoritative name servers\nA server may be an authoritative source for zero or more \nzones\nData for a zone is entered into a local master file\nMaster (primary) server reads the zone data directly from the \nmaster file\nSecondary authoritative servers download zone data from \nprimary server\nSecondary servers periodically check their version number \nagainst the master server\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide51",
    "text": "DNS server functions and configuration\nMain function is to resolve domain names for \ncomputers, i.e. to get their IP addresses\ncaches the results of previous searches until they pass \ntheir 'time to live'\nOther functions:\nget mail host for a domain \nreverse resolution - get domain name from IP address\nHost information - type of hardware and OS\nWell-known services - a list of services offered by a host\nOther attributes can be included (optional)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide52",
    "text": "Caching in DNS\nAny server  can cache any name\nNon-authoritative servers note time-to-live when they \ncache data\nNon-authoritative servers indicate that they are such \nwhen responding to clients with cached names\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide53",
    "text": "DNS clients (resolvers)\nResolvers are usually implemented as library \nroutines (e.g., gethostbyname).\nThe request is formatted into a DNS record.\nDNS servers use a well-known port.\nA request-reply protocol is used \nTCP or UDP why?\nThe resolver times out and resends if it doesn’t \nreceive a response in a specified time.\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide54",
    "text": "DNS name resolution\nDomain name à IP address ???\nLook for the name in the local cache\nTry a superior DNS server, which responds with:\nthe IP address (which may not be entirely up to date)\nOr, another recommended DNS server (iterative)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide55",
    "text": "DNS name servers\nNote: Name server names are in \nitalics, and the corresponding \ndomains are in parentheses.\nArrows denote name server \nentries  \na.root-servers.net\n(root)\nns0.ja.net\n(ac.uk)\ndns0.dcs.qmw.ac.uk\n(dcs.qmw.ac.uk)\nalpha.qmw.ac.uk\n(qmw.ac.uk)\ndns0-doc.ic.ac.uk\n(ic.ac.uk)\nns.purdue.edu\n(purdue.edu)\nuk\npurdue.edu\nic.ac.uk\nqmw.ac.uk\n...\ndcs.qmw.ac.uk\n*.qmw.ac.uk\n*.ic.ac.uk\n*.dcs.qmw.ac.uk\n* .purdue.edu\nns1.nic.uk\n(uk)\nac.uk\n...\nco.uk\nyahoo.com\n....\nauthoritative path to lookup:\njeans-pc.dcs.qmw.ac.uk\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide56",
    "text": "DNS in typical operation\na.root-servers.net\n(root)\nns0.ja.net\n(ac.uk)\ndns0.dcs.qmw.ac.uk\n(dcs.qmw.ac.uk)\nalpha.qmw.ac.uk\n(qmw.ac.uk)\ndns0-doc.ic.ac.uk\n(ic.ac.uk)\nns.purdue.edu\n(purdue.edu)\nuk\npurdue.edu\nic.ac.uk\nqmw.ac.uk\n...\ndcs.qmw.ac.uk\n*.qmw.ac.uk\n*.ic.ac.uk\n*.dcs.qmw.ac.uk\n* .purdue.edu\nns1.nic.uk\n(uk)\nac.uk\n...\nco.uk\nyahoo.com\n....\nclient.ic.ac.uk\nIP: alpha.qmw.ac.uk\n2\n3\nIP:dns0.dcs.qmw.ac.uk\njeans-pc.dcs.qmw.ac.uk ?\nIP:ns0.ja.net\n1\nIP:jeans-pc.dcs.qmw.ac.uk \n4\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide57",
    "text": "DNS issues\nName tables change infrequently, but when they do, \ncaching can result in the delivery of stale data.\n\nClients are responsible for detecting this and recovering\nIts design makes changes to the structure of the name \nspace difficult. For example:\n\nmerging previously separate domain trees under a new root\n\nmoving sub-trees to a different part of the structure \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide58",
    "text": "Attribute-Based Naming\nAlso known as Directory Services  \nIn many cases, it is much more convenient to name, and \nlook up entities by means of their attributes (e.g., look for a \nstudent who got A in OS)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide59",
    "text": "Directory Services\n\nEntities have a set of attributes (e.g., email: send, recv, subject, ...)\n\nIn most cases, attributes are determined manually\n\nSetting values consistently is a crucial problem ...\n\nOften organized in a hierarchy\n\nExamples of directory services: X.500, Microsoft’s Active Directory \nServices,\nThen, look up entities by means of their attributes\nProblem: Lookup operations can be extremely expensive, \nas they require to match requested attribute values, against \nactual attribute values \n\nIn the simplest form, inspect all entities.\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide60",
    "text": "Directory Services (cont’d)\nSolutions: \nLightweight Directory Access Protocol (LDAP): \n\nImplement basic directory service as database, and Combine it \nwith traditional structured naming system.\n\nDerived from OSI’s X.500 directory service, which maps a person’s \nname to attributes (email address, etc.)\nDHT-based decentralized implementation \n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide61",
    "text": "Hierarchical implementation: LDAP (1)\nLDAP directory service consists of a set of records \nEach directory entry (record) is made up of a set of \n(Attribute, Value(s)) pairs \nn Collection of all directory \nentries is called Directory \nInformation Base (DIB)\nn Each record is uniquely named by using naming \nattributes in the record (e.g., first five in the above record)\nn Each naming attribute is called relative distinguished \nname (RDN)\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide62",
    "text": "Hierarchical implementation: LDAP (2)\nWe can create a directory information tree (DIT) by listing \nRDNs in sequence\nanswer =\nsearch(\"&(C = NL) (O = Vrije Universiteit) \n(OU = *) (CN = Main server)\")\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide63",
    "text": "Hierarchical implementation: LDAP (3)\nClients called Directory User Agent \n(DUA), similar to name resolver and \ncontacts the server\nLDAP server known as Directory \nService Agent (DSA) maintains DIT and \nlooks up entries based on attr.\nIn case of a large scale directory, DIT is \npartitioned and distribute across several \nDSAs\nImplementation of LDAP is similar to \nDNS, but LDAP provides more \nadvanced lookup operations \nLDAP \nClient\nLDAP \nServer\nX.500 \nDirectory \nServer\nClient \nrequest \nin  LDAP\nRequest \nin  X.500\nResponse\n in  X.500\nServer \nresponse \nin  LDAP\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide64",
    "text": "66\nHierarchical implementation: LDAP (4)\nSimple DUA interface to X.500 \nLDAP runs over TCP/IP\nUses textual encoding\nProvides secure access through authentication\nOther directory services have implemented it\nSee RFC 2251 [Wahl et al. 1997]\n"
  },
  {
    "doc": "lecture06_processed.json",
    "id": "slide65",
    "text": "LDAP Evolution\n\nUniversity of Michigan added to LDAP servers the capability of \naccessing own database.\n\nUse of LDAP databases became widespread\n\nSchemes were developed for registering changes and exchanging \ndeltas between LDAP servers\n\nIn 1996 three engineers from U of Michigan joined Netscape. 40 \ncompanies (w/o Microsoft) announced support of LDAP as the \nstandard for directory services\n\nCore specifications for LDAPv3 was published as IETF RFCs 2251-\n2256.\n"
  }
]