[
  {
    "doc": "06",
    "slide": "slide1",
    "text": "Naming in Distributed Systems\n\u2022 Concepts\n\u2022 Flat Naming\n\u2022 Structured Naming\n\u2022 Attribute-based Naming"
  },
  {
    "doc": "06",
    "slide": "slide2",
    "text": "NAMING\n\uf07aNAMES, IDENTIFIERS, AND ADDRESSES \n\uf07aFLAT NAMING \n\uf079\nSimple Solutions \n\uf079\nHome-Based Approaches \n\uf079\nDistributed Hash Tables (More in P2P)\n\uf079\nHierarchical Approaches \n\uf07aSTRUCTURED NAMING \n\uf079\nName Spaces \n\uf079\nName Resolution \n\uf079\nThe Implementation of a Name Space \n\uf079\nExample: The Domain Name System \n\uf07aATTRIBUTE-BASED NAMING \n\uf079\nDirectory Services \n\uf079\nHierarchical Implementations: LDAP"
  },
  {
    "doc": "06",
    "slide": "slide3",
    "text": "Objectives\n\uf07aTo understand naming and related issues in DS\n\uf07aTo learn naming space and implementation\n\uf07aTo learn flat and structured names and how \nthey are resolved\n\uf07aTo learn Attributed-based naming"
  },
  {
    "doc": "06",
    "slide": "slide4",
    "text": "What a Name is in DS?\n\uf07aA name is a string of bits or characters that is used to refer \nto an entity (an entity could be anything  such as host, printer, file, process, \nmailbox, user etc.) \n\uf07aTo operate on an entity, we need to access it, for which we \nneed an access point.\n\uf07aAccess point is a special kind of entity and its name is called \nan address (address of the entity, e.g., IP, port #, phone #)\n\uf079\nAn entity may have more than one access point/address\n\uf079\nAn entity may change its access points/a"
  },
  {
    "doc": "06",
    "slide": "slide5",
    "text": "Identifier\n\uf07aA special name to uniquely identify an entity (SSN, MAC)\n\uf07aA true identifier has the following three properties:\n\uf079\nP1: Each identifier refers to at most one entity\n\uf079\nP2: Each entity is referred to by at most one identifier\n\uf079\nP3: An identifier always refers to same entity (no reuse)\n \n \nAddresses \n \nEntities \n \n \nIdentifiers\nI1\nI2\nI3\nE1\nE2\nE3\nA1\nA2\nA3\nA4\nAddresses and identifiers are important and used for different purposes, but \nthey are often represented in machine readable format ("
  },
  {
    "doc": "06",
    "slide": "slide6",
    "text": "Human-friendly names\n\uf07aFile names, variable names etc. are human-friendly names \ngiven to each entity\n\uf07aQuestion: how to map/resolve these names to addresses \nso that we can access the entities on which we want to \noperate?\n\uf07aSolution: have a naming system that maintains name-to-\naddress binding!\n\uf07aThe simplest form is to have a centralized table!\n\uf079\nWhy or why not this will work?\n\uf07aWe will study three different naming systems and how they \nmaintain such a table in a distributed manner!"
  },
  {
    "doc": "06",
    "slide": "slide7",
    "text": "Naming Systems and Their Goals\n\uf07aNaming Systems \n\uf079Flat names\n\uf079Structured names\n\uf079Attributed-based names\n\uf07aGoals\n\uf079Scalable to arbitrary size\n\uf079Have a long lifetime\n\uf079Be highly available\n\uf079Have fault isolation\n\uf079Tolerate mistrust"
  },
  {
    "doc": "06",
    "slide": "slide8",
    "text": "Flat Names"
  },
  {
    "doc": "06",
    "slide": "slide9",
    "text": "Flat Naming\n\uf07aFlat name: random bits of string, no structure\n\uf079\nE.g., SSN, MAC address\n\uf07aResolution problem: \n \nGiven a flat (unstructured) name, how can we find/locate \nits associated access point and its address?\n\uf07aSolutions:\n\uf079\nSimple solutions (broadcasting)\n\uf079\nHome-based approaches\n\uf079\nDistributed Hash Tables (structured P2P)\n\uf079\nHierarchical location service"
  },
  {
    "doc": "06",
    "slide": "slide10",
    "text": "Simple Solution: Broadcasting \n\uf07aSimply broadcast the target ID to every entity\n\uf07aEach entity compares the requested ID with its own ID\n\uf07aThe target entity returns its current address\n\uf07aExample: \n\uf079\nRecall ARP in LAN\n\uf07aAdv/Disadvantages\n\uf079\n+ simple\n\uf079\n- not scale beyond LANs \n\uf079\n- it requires all entities to listen to all incoming requests\nWho has the address \n192.168.0.1?\nI am 192.168.0.1. My identifier \nis 02:AB:4A:3C:59:85"
  },
  {
    "doc": "06",
    "slide": "slide11",
    "text": "Forwarding Pointers\n\uf079Stub-Scion Pair (SSP) chains implement remote invocations for mobile \nentities using forwarding pointers\n\uf079\nServer stub is referred to as Scion in the original paper\n\uf079Each forwarding pointer is implemented as a pair: \n \n \n(client stub, server stub)\n\uf079\nThe server stub contains a local reference to the actual object or a local \nreference to another client stub\n\uf079When object moves from A (e.g., P2) to B (e.g., P3), \n\uf079\nIt leaves a client stub at A (i.e., P2)\n\uf079\nIt installs a server "
  },
  {
    "doc": "06",
    "slide": "slide12",
    "text": "Forwarding Pointers\nHow to locate mobile entities?\n\uf07aWhen an entity moves from A to B, leaves a pointer to A \nthat it is at B now\u2026 \n\uf07aDereferencing: simply follow the chain of pointers and \nmake this entirely transparent to clients\n\uf07aAdv/Disadvantages\n\uf079\n+ support for mobile nodes\n\uf079\n- geographical scalability problems\n\uf079\n- long chains are not fault tolerant\n\uf079\n- increased network latency"
  },
  {
    "doc": "06",
    "slide": "slide13",
    "text": "Home-Based Approaches\nHow to deal with scalability problem when locating mobile entities?\n\uf07a\nLet a home keep track of where the entity is!\n\uf07a\nHow will the clients continue to communicate? \n\uf079\nHome agent gives the new location to the client so it can directly \ncommunicate\n\uf078\nefficient but not transparent\n\uf079\nHome agent forwards the messages to new location\n\uf078\nTransparent but may not be efficient"
  },
  {
    "doc": "06",
    "slide": "slide14",
    "text": "Home-Based Approaches: An Example\nHome node\nMobile entity\n1. Update home node about the \nforeign address\n2. Client sends the packet to the \nmobile entity at its home node\n3a. Home node forwards the \nmessage to the foreign address \nof the mobile entity\n3b. Home node replies the client \nwith the  current IP address of \nthe mobile entity\n4. Client directly sends all \nsubsequent packets directly to the \nforeign address of the mobile entity\nFrom CS15-440 CMU Qatar, Hammoud"
  },
  {
    "doc": "06",
    "slide": "slide15",
    "text": "15\nProblems with home-based approaches\n\uf07aThe home address has to be supported as long as the \nentity lives.\n\uf07aThe home address is fixed, which means an \nunnecessary burden when the entity permanently \nmoves to another location\n\uf079\nHow can we solve the \u201cpermanent move\u201d problem?\n\uf07aPoor geographical scalability (the entity may be next to \nthe client)"
  },
  {
    "doc": "06",
    "slide": "slide16",
    "text": "Next: Distributed Hash Table\n\uf07aDistributed Hash Table \u2013 In a nutshell\nhttps://www.youtube.com/watch?v=tz-Q-eW8FbQ"
  },
  {
    "doc": "06",
    "slide": "slide17",
    "text": "Distributed Hash Tables\nHow to use DHT to resolve flat ID\n\uf07aMany nodes into a logical ring\n\uf079\nEach node is assigned a random m-bit identifier.\n\uf079\nEvery entity is assigned a unique m-bit key.\n\uf079\nEntity with key k falls under jurisdiction of node with smallest id >= k \n(called its successor)\n\uf07aLinearly resolve a key k to the address of succ(k)\n\uf079\nEach node p keeps two neighbors: \n \n succ(p+1) and pred(p)\n\uf079\nIf k > p  then \n \n  \nforward to succ(p+1)\n\uf079\nif k <= pred(p) then \n \n  \nforward k to pred(p)\n\uf079\nIf p"
  },
  {
    "doc": "06",
    "slide": "slide18",
    "text": "Chord\n\uf07a\nChord assigns an m-bit identifier (randomly \nchosen) to each node\n\uf079\nA node can be contacted through its network \naddress\n\uf07a\nAlongside, it maps each entity to a node\n\uf079\nEntities can be processes, files, etc.,\n\uf07a\nMapping of entities to nodes\n\uf079\nEach node is responsible for a set of entities\n\uf079\nAn entity with key k falls under the jurisdiction of the \nnode with the smallest identifier id >= k. This \nnode is known as the successor of k, and is \ndenoted by succ(k)\nNode 000\nNode 005\nNode 010\nNode  "
  },
  {
    "doc": "06",
    "slide": "slide19",
    "text": "A Na\u00efve Key Resolution Algorithm - Linearly\n\uf07a\nThe main issue in DHT is to efficiently resolve a key k to the network location of succ(k)\n\uf079\nGiven an entity with key k, how to find the node succ(k)?\n1. All nodes are arranged in a logical \nring according to their IDs\n2. Each node \u2018p\u2019 keeps track of its \nimmediate neighbors: succ(p) and \npred(p)\n3. If \u2018p\u2019 receives a request to resolve \nkey \u2018k\u2019:\n\u2022 If pred(p) < k <=p, node p \nwill handle it\n\u2022 Else it will forward it to succ(n) \nor pred(n)\nn\n= Active n"
  },
  {
    "doc": "06",
    "slide": "slide20",
    "text": "\uf07a\nChord improves key resolution by reducing the time complexity to O(log n)\n\uf07a\nAll nodes are arranged in a logical ring according to their IDs\n\uf07a\nEach node \u2018p\u2019 keeps a table FTp of at-most m entries. This table is Finger Table\n      FTp[i] = succ(p + 2(i-1))\nNOTE: FTp[i] increases exponentially\n\uf07a\nIf node \u2018p\u2019 receives a request to resolve key \u2018k\u2019:\n\u2022 Node p will forward it to node q with index j in Fp where\n  q = FTp[j] <= k < FTp[j+1]\n\u2022 If k > FTp[m], then node p will forward it to FTp[m]\n\u2022 If k < "
  },
  {
    "doc": "06",
    "slide": "slide21",
    "text": "Chord DHT Example\n1\n04\n2\n04\n3\n09\n4\n09\n5\n18\n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n1\n09\n2\n09\n3\n09\n4\n14\n5\n20\n1\n11\n2\n11\n3\n14\n4\n18\n5\n28\n1\n14\n2\n14\n3\n18\n4\n20\n5\n28\n1\n18\n2\n18\n3\n18\n4\n28\n5\n01\n1\n21\n2\n28\n3\n28\n4\n28\n5\n04\n1\n28\n2\n28\n3\n28\n4\n01\n5\n09\n1\n01\n2\n01\n3\n01\n4\n04\n5\n14\ni\nsucc(p + 2(i-1))\n26\n1.\nAll nodes are arranged in a logical \nring according to their IDs\n2.\nEach node \u2018p\u2019 keeps a table FTp of \nat-most m entries. This table is called \nFinger Table\n   "
  },
  {
    "doc": "06",
    "slide": "slide22",
    "text": "DHT: Finger Table (cont\u2019d)\n\uf07a\nHow to handle\n\uf079\nJoin\n\uf078\nIt contacts arbitrary node, looks up for succ(p+1), and inserts itself into the ring\n\uf079\nLeave \n\uf078\nIt contacts pred(p) and succ(p+1) and updates them\n\uf079\nFail \n00\n01\n02\n03\n04\n05\n06\n07\n08\n09\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n02\nWho is \nsucc(2+1) ?\nNode 4 is \nsucc(2+1)"
  },
  {
    "doc": "06",
    "slide": "slide23",
    "text": "DHT: Finger Table (cont\u2019d)\n\uf07a\nThe complexity comes from keeping the finger tables up to date\n\uf07a\nBy-and-large Chord tries to keep them consistent\n\uf079\nBut a simple mechanism may lead to performance problems\n\uf079\nTo fix this we need to exploit network proximity when assigning node ID"
  },
  {
    "doc": "06",
    "slide": "slide24",
    "text": "Exploiting network proximity\nProblem: The logical organization of nodes in the overlay may lead to \nerratic message transfers in the underlying Internet: node k and node \nsucc(k +1) may be very far apart.\n\uf07a\nTopology-aware node assignment: \n \nWhen assigning an ID to a node, make sure that nodes close in the ID space are also \nclose in the network. Can be very difficult.\n\uf07a\nProximity routing: \n \nMaintain more than one possible successor, and forward to the closest.\n \nExample: in Chord FTp[i] points"
  },
  {
    "doc": "06",
    "slide": "slide25",
    "text": "Hierarchical Location Services (HLS)\nA leaf domain, contained in S\nDirectory node  dir(S) of \ndomain S\nA subdomain S\nof top-level domain T  (S is \ncontained in T)\nTop-level  \ndomain T\nBasic idea\nBuild a large-scale search tree for which the underlying network is  \ndivided into hierarchical domains. Each domain is represented by a  \nseparate directory node.\nPrinciple\nnode dir(T)\nThe root directory"
  },
  {
    "doc": "06",
    "slide": "slide26",
    "text": "HLS: Tree organization\nDomain D2\nDomain D1\nLocation record  with \nonly one field,\ncontaining an address\nInvariants\n\u25baAddress of entity E is stored in a leaf or intermediate node\n\u25baIntermediate nodes contain a pointer to a child if and only if the subtree \nrooted at the child stores an address of the entity\n\u25baThe root knows about all entities\nStoring information of an entity having two addresses in  different leaf \ndomains\ndom(N) with  \npointer to N\nLocation record  for E at \nnode M\nN\nField with no "
  },
  {
    "doc": "06",
    "slide": "slide27",
    "text": "HLS: Lookup operation\nBasic principles\n\u25baStart lookup at local leaf node\n\u25baNode knows about E \u21d2 follow downward pointer, else go up\n\u25baUpward lookup always stops at root\nLooking up a location\nDomain D\nM\nNode has no  \nrecord for E, so  \nthat request is  \nforwarded to  \nparent\nLook-up  \nrequest\nNode knows\nabout E, so request  \nis forwarded to child"
  },
  {
    "doc": "06",
    "slide": "slide28",
    "text": "Structured Naming"
  },
  {
    "doc": "06",
    "slide": "slide29",
    "text": "Name Space \nCollection of valid names\n\uf07aA directed graph with two types of nodes\n\uf079\nLeaf node represents a (named) entity, has no outgoing link, and stores \ninformation about the entity (e.g., address)\n\uf079\nA directory node is an entity that refers to other nodes: contains a \n(directory) table of (edge label, node identifier) pairs\nroot"
  },
  {
    "doc": "06",
    "slide": "slide30",
    "text": "Name Space  (cont.)\n\uf07aEach node in the graph is actually considered to be \nanother entity and we can easily store all kinds of \nattributes in a node, describing aspects of the entity the \nnode represents:\n\uf079\nType of the entity\n\uf079\nAn identifier for that entity \n\uf079\nAddress of the entity\u2019s location \n\uf079\nNicknames\n\uf079\n\u2026 \u2026"
  },
  {
    "doc": "06",
    "slide": "slide31",
    "text": "Name Resolution\nlooking up a name\nN: <label-1, label-2, \u2026, label-n>\n \nStart at directory node N\n \n \nfind label-1 in directory table of N\n \n \nget the identifier\n \n \ncontinue resolving at that node until reaching label-n\n\uf07a\nProblem: where to start? How do we actually find that (initial) node?\n\uf07a\nClosure mechanism:  knowing how and where to start name \nresolution. It is always implicit. Why? \n\uf079\nInode in unix is the first block in logical disk\n\uf079\nwww.cs.vu.nl: start at a DNS name server\n\uf079\n/home/steen/m"
  },
  {
    "doc": "06",
    "slide": "slide32",
    "text": "Name Resolution: Aliases and linking\n\uf07aAlias is another name for the same entity.\n\uf07aThere are 2 ways of aliasing in naming graphs\n\uf079\nHard Links: What we have described so far as a path name: a name that is \nresolved by following a specific path in a naming graph from one node to \nanother (i.e., there are more than one absolute paths to a certain node) \n\uf079\nSoft Links: We can represent an entity by a leaf node that stores an \nabsolute path name of another node. (like symbolic links in UNIX file \nsyste"
  },
  {
    "doc": "06",
    "slide": "slide33",
    "text": "Name Linking\n\uf07aThe name space can be effectively used to link two \ndifferent entities\n\uf07aTwo types of links can exist between the nodes:\n1. Hard Links\n2. Symbolic Links"
  },
  {
    "doc": "06",
    "slide": "slide34",
    "text": "1. Hard Links\n\uf079There is a directed link from \nthe hard link to the actual \nnode\n\uf079Name resolution:\n\uf078Similar to the general \nname resolution\n\uf079Constraint:\n\uf078There should be no cycles \nin the graph\n\u201c/home/steen/keys\u201d is a hard link \nto \u201c/keys\u201d\nn0\nn1\nn4\nn5\nn2\nn3\nhome\nkeys\nsteen\nmax\nelke\n\u201c/keys\u201d\ntwmrc\nmbox\nkeys"
  },
  {
    "doc": "06",
    "slide": "slide35",
    "text": "2. Symbolic Links\n\uf079Symbolic link stores the name of \nthe original node as data\n\uf079Name resolution for a symbolic link \nSL\n\uf078First resolve SL\u2019s name\n\uf078Read the content of SL \n\uf078Name resolution continues with \ncontent of SL\n\uf079Constraint:\n\uf078No cyclic references should be \npresent\n\u201c/home/steen/keys\u201d is a \nsymbolic link to \u201c/keys\u201d\nn0\nn1\nn4\nn5\nn2\nn3\nhome\nkeys\nsteen\nmax\nelke\n\u201c/keys\u201d\ntwmrc mbox\nkeys\nn6\n\u201c/keys\u201d\nData stored in n6"
  },
  {
    "doc": "06",
    "slide": "slide36",
    "text": "Mounting of Name Spaces\n\uf07a\nTwo or more name spaces can be merged transparently by a \ntechnique known as mounting\n\uf07a\nWith mounting, a directory node in one name space will store the \nidentifier of the directory node of another name space\n\uf07a\nNetwork File System (NFS) is an example where different name \nspaces are mounted\n\uf079\nNFS enables transparent access to remote files"
  },
  {
    "doc": "06",
    "slide": "slide37",
    "text": "Example of Mounting Name Spaces in NFS\nMachine B\nName Space 2\nOS\nMachine A\nName Space 1\nOS\nhome\nsteen\nmbox\nName Server \nfor \nforeign name \nspace\nremot\ne\nvu\n\u201cnfs://flits.cs.vu.\nnl/home/steen\u201d\nName resolution for \u201c/remote/vu/home/steen/mbox\u201d in a \ndistributed file system"
  },
  {
    "doc": "06",
    "slide": "slide38",
    "text": "Name Space \nImplementation\nDistributed vs. centralized"
  },
  {
    "doc": "06",
    "slide": "slide39",
    "text": "\uf07aBasic issue: Distribute the name resolution process as \nwell as name space management across multiple \nmachines, by distributing nodes of the naming graph\n\uf07aLarge name spaces are organized in a hierarchical way. \nThere are three logical layers\n\uf079\nGlobal level: Consists of the high-level directory nodes \nrepresenting different organizations or groups\n\uf078\nStable (directory tables don\u2019t change often)\n\uf078\nHave to be jointly managed by different administrations\n\uf079\nAdministrational level: Contains mid-level"
  },
  {
    "doc": "06",
    "slide": "slide40",
    "text": "Name Space Distribution (cont.)"
  },
  {
    "doc": "06",
    "slide": "slide41",
    "text": "Name Space Distribution (cont.)\n\uf07aServers in each layer have different \nrequirements regarding availability and \nperformance\nAvailability\nPerformance\nGlobal\nMust be very high\nReplication may \nhelp\nCan be cached (stability)\nReplication may help\nAdministrat\nive\nMust be very high \nparticularly for the \nclients in the same \norganization\nLooks up should be fast\nUse high-end machines\nManagerial\nLess demanding\nOne dedicated \nserver might be \nenough\nPerformance is crucial\nOperations should take \nplace im"
  },
  {
    "doc": "06",
    "slide": "slide42",
    "text": "Implementation of Name Resolution\nIterative \n \n \n   vs. \n \nRecursive\n-Caching is restricted to client\n-Communication cost, Delay \n+ less overhead on root\n+Caching can be more effective\n+Communication cost might be reduced\n- Too much overhead on root"
  },
  {
    "doc": "06",
    "slide": "slide43",
    "text": "Cache in Recursive Naming Resolution\n\uf07aRecursive name resolution of <nl, vu, cs, ftp>. \n\uf07aName servers cache intermediate results for subsequent \nlookups"
  },
  {
    "doc": "06",
    "slide": "slide44",
    "text": "Scalability Issues\n\uf07a\nSize scalability: We need to ensure that servers can handle a large \nnumber of requests per time unit \u00e0 high-level servers are in big \ntrouble.\n\uf079\nSolution: Assume (at least at global and administrational level) that \ncontent of nodes hardly ever changes. In that case, we can apply \nextensive replication by mapping nodes to multiple servers, and start \nname resolution at the nearest server.\n\uf07a\nGeographical scalability: We need to ensure that the name \nresolution process scales"
  },
  {
    "doc": "06",
    "slide": "slide45",
    "text": "Case Study: Domain \nName System (DNS)"
  },
  {
    "doc": "06",
    "slide": "slide46",
    "text": "Case Study: Domain Name System (DNS)\n\uf07aOne of the largest distributed naming database/service\n\uf07aThe DNS name space is hierarchically organized as a \nrooted tree. Name structure reflects administrative \nstructure of the Internet\n\uf07aRapidly resolves domain names \nto IP addresses\n\uf079\nexploits caching heavily\n\uf079\ntypical query time ~100 milliseconds\n\uf07aScales to millions of computers\n\uf079\npartitioned database\n\uf079\ncaching\n\uf07aResilient to failure of a server\n\uf079\nreplication"
  },
  {
    "doc": "06",
    "slide": "slide47",
    "text": "Domain names (last element of name)\n\uf07a\ncom - commercial organizations\n\uf07a\nedu - universities and educational institutions\n\uf07a\ngov - US government agencies\n\uf07a\nmil - US military organizations\n\uf07a\nnet - major network support centers\n\uf07a\norg - organizations not included in first five\n\uf07a\nint - international organization\n\uf07a\ncountry codes - (e.g., cn, us, uk, fr, etc.)"
  },
  {
    "doc": "06",
    "slide": "slide48",
    "text": "Name spaces in DNS\n\uf07aHierarchical structure - one or more components \nor labels separated by periods (.)\n\uf07aOnly absolute names - referred relative to global \nroot\n\uf07aClients usually have a list of default domains that \nare appended to single-component domain \nnames before trying global root"
  },
  {
    "doc": "06",
    "slide": "slide49",
    "text": "Zone partitioning of DNS name space\n\uf07aZone - contains attribute data for names in domain minus \nthe sub-domains administrated by lower-level authorities:\n\uf07aNames of the servers for the sub-domains\n\uf07aAt least two name servers that provide authoritative \ndata for the zone\n\uf07aZone management parameters: cache, replication"
  },
  {
    "doc": "06",
    "slide": "slide50",
    "text": "Authoritative name servers\n\uf07aA server may be an authoritative source for zero or more \nzones\n\uf07aData for a zone is entered into a local master file\n\uf07aMaster (primary) server reads the zone data directly from the \nmaster file\n\uf07aSecondary authoritative servers download zone data from \nprimary server\n\uf07aSecondary servers periodically check their version number \nagainst the master server"
  },
  {
    "doc": "06",
    "slide": "slide51",
    "text": "DNS server functions and configuration\n\uf07aMain function is to resolve domain names for \ncomputers, i.e. to get their IP addresses\n\uf079caches the results of previous searches until they pass \ntheir 'time to live'\n\uf07aOther functions:\n\uf079get mail host for a domain \n\uf079reverse resolution - get domain name from IP address\n\uf079Host information - type of hardware and OS\n\uf079Well-known services - a list of services offered by a host\n\uf079Other attributes can be included (optional)"
  },
  {
    "doc": "06",
    "slide": "slide52",
    "text": "Caching in DNS\n\uf07aAny server  can cache any name\n\uf07aNon-authoritative servers note time-to-live when they \ncache data\n\uf07aNon-authoritative servers indicate that they are such \nwhen responding to clients with cached names"
  },
  {
    "doc": "06",
    "slide": "slide53",
    "text": "DNS clients (resolvers)\n\uf07aResolvers are usually implemented as library \nroutines (e.g., gethostbyname).\n\uf07aThe request is formatted into a DNS record.\n\uf07aDNS servers use a well-known port.\n\uf07aA request-reply protocol is used \n\uf079TCP or UDP why?\n\uf07aThe resolver times out and resends if it doesn\u2019t \nreceive a response in a specified time."
  },
  {
    "doc": "06",
    "slide": "slide54",
    "text": "DNS name resolution\n\uf07aDomain name \u00e0 IP address ???\n\uf07aLook for the name in the local cache\n\uf07aTry a superior DNS server, which responds with:\n\uf079the IP address (which may not be entirely up to date)\n\uf079Or, another recommended DNS server (iterative)"
  },
  {
    "doc": "06",
    "slide": "slide55",
    "text": "DNS name servers\nNote: Name server names are in \nitalics, and the corresponding \ndomains are in parentheses.\nArrows denote name server \nentries  \na.root-servers.net\n(root)\nns0.ja.net\n(ac.uk)\ndns0.dcs.qmw.ac.uk\n(dcs.qmw.ac.uk)\nalpha.qmw.ac.uk\n(qmw.ac.uk)\ndns0-doc.ic.ac.uk\n(ic.ac.uk)\nns.purdue.edu\n(purdue.edu)\nuk\npurdue.edu\nic.ac.uk\nqmw.ac.uk\n...\ndcs.qmw.ac.uk\n*.qmw.ac.uk\n*.ic.ac.uk\n*.dcs.qmw.ac.uk\n* .purdue.edu\nns1.nic.uk\n(uk)\nac.uk\n...\nco.uk\nyahoo.com\n....\nauthoritative path to lookup:\njeans-pc."
  },
  {
    "doc": "06",
    "slide": "slide56",
    "text": "DNS in typical operation\na.root-servers.net\n(root)\nns0.ja.net\n(ac.uk)\ndns0.dcs.qmw.ac.uk\n(dcs.qmw.ac.uk)\nalpha.qmw.ac.uk\n(qmw.ac.uk)\ndns0-doc.ic.ac.uk\n(ic.ac.uk)\nns.purdue.edu\n(purdue.edu)\nuk\npurdue.edu\nic.ac.uk\nqmw.ac.uk\n...\ndcs.qmw.ac.uk\n*.qmw.ac.uk\n*.ic.ac.uk\n*.dcs.qmw.ac.uk\n* .purdue.edu\nns1.nic.uk\n(uk)\nac.uk\n...\nco.uk\nyahoo.com\n....\nclient.ic.ac.uk\nIP: alpha.qmw.ac.uk\n2\n3\nIP:dns0.dcs.qmw.ac.uk\njeans-pc.dcs.qmw.ac.uk ?\nIP:ns0.ja.net\n1\nIP:jeans-pc.dcs.qmw.ac.uk \n4"
  },
  {
    "doc": "06",
    "slide": "slide57",
    "text": "DNS issues\n\uf07aName tables change infrequently, but when they do, \ncaching can result in the delivery of stale data.\n\uf079\nClients are responsible for detecting this and recovering\n\uf07aIts design makes changes to the structure of the name \nspace difficult. For example:\n\uf079\nmerging previously separate domain trees under a new root\n\uf079\nmoving sub-trees to a different part of the structure"
  },
  {
    "doc": "06",
    "slide": "slide58",
    "text": "Attribute-Based Naming\nAlso known as Directory Services  \nIn many cases, it is much more convenient to name, and \nlook up entities by means of their attributes (e.g., look for a \nstudent who got A in OS)"
  },
  {
    "doc": "06",
    "slide": "slide59",
    "text": "Directory Services\n\uf07a\nEntities have a set of attributes (e.g., email: send, recv, subject, ...)\n\uf07a\nIn most cases, attributes are determined manually\n\uf07a\nSetting values consistently is a crucial problem ...\n\uf07a\nOften organized in a hierarchy\n\uf079\nExamples of directory services: X.500, Microsoft\u2019s Active Directory \nServices,\n\uf07aThen, look up entities by means of their attributes\n\uf07aProblem: Lookup operations can be extremely expensive, \nas they require to match requested attribute values, against \nactual attri"
  },
  {
    "doc": "06",
    "slide": "slide60",
    "text": "Directory Services (cont\u2019d)\nSolutions: \n\uf07aLightweight Directory Access Protocol (LDAP): \n\uf079\nImplement basic directory service as database, and Combine it \nwith traditional structured naming system.\n\uf079\nDerived from OSI\u2019s X.500 directory service, which maps a person\u2019s \nname to attributes (email address, etc.)\n\uf07aDHT-based decentralized implementation"
  },
  {
    "doc": "06",
    "slide": "slide61",
    "text": "Hierarchical implementation: LDAP (1)\n\uf07aLDAP directory service consists of a set of records \n\uf07aEach directory entry (record) is made up of a set of \n(Attribute, Value(s)) pairs \nn Collection of all directory \nentries is called Directory \nInformation Base (DIB)\nn Each record is uniquely named by using naming \nattributes in the record (e.g., first five in the above record)\nn Each naming attribute is called relative distinguished \nname (RDN)"
  },
  {
    "doc": "06",
    "slide": "slide62",
    "text": "Hierarchical implementation: LDAP (2)\n\uf07aWe can create a directory information tree (DIT) by listing \nRDNs in sequence\nanswer =\nsearch(\"&(C = NL) (O = Vrije Universiteit) \n(OU = *) (CN = Main server)\")"
  },
  {
    "doc": "06",
    "slide": "slide63",
    "text": "Hierarchical implementation: LDAP (3)\n\uf07aClients called Directory User Agent \n(DUA), similar to name resolver and \ncontacts the server\n\uf07aLDAP server known as Directory \nService Agent (DSA) maintains DIT and \nlooks up entries based on attr.\n\uf07aIn case of a large scale directory, DIT is \npartitioned and distribute across several \nDSAs\n\uf07aImplementation of LDAP is similar to \nDNS, but LDAP provides more \nadvanced lookup operations \nLDAP \nClient\nLDAP \nServer\nX.500 \nDirectory \nServer\nClient \nrequest \nin  LD"
  },
  {
    "doc": "06",
    "slide": "slide64",
    "text": "66\nHierarchical implementation: LDAP (4)\n\uf07aSimple DUA interface to X.500 \n\uf07aLDAP runs over TCP/IP\n\uf07aUses textual encoding\n\uf07aProvides secure access through authentication\n\uf07aOther directory services have implemented it\n\uf07aSee RFC 2251 [Wahl et al. 1997]"
  },
  {
    "doc": "06",
    "slide": "slide65",
    "text": "LDAP Evolution\n\uf07a\nUniversity of Michigan added to LDAP servers the capability of \naccessing own database.\n\uf07a\nUse of LDAP databases became widespread\n\uf07a\nSchemes were developed for registering changes and exchanging \ndeltas between LDAP servers\n\uf07a\nIn 1996 three engineers from U of Michigan joined Netscape. 40 \ncompanies (w/o Microsoft) announced support of LDAP as the \nstandard for directory services\n\uf07a\nCore specifications for LDAPv3 was published as IETF RFCs 2251-\n2256."
  }
]